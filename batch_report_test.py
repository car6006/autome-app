#!/usr/bin/env python3
"""
Batch Report Functionality Test
Comprehensive testing of the /api/notes/batch-report endpoint
"""

import requests
import sys
import json
import time
from datetime import datetime
import tempfile
import os

class BatchReportTester:
    def __init__(self, base_url="https://typescript-auth.preview.emergentagent.com"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.tests_run = 0
        self.tests_passed = 0
        self.created_notes = []
        self.auth_token = None
        self.expeditors_token = None
        self.test_user_data = {
            "email": f"batch_test_{int(time.time())}@example.com",
            "username": f"batchuser_{int(time.time())}",
            "password": "BatchTest123!",
            "first_name": "Batch",
            "last_name": "Tester"
        }
        self.expeditors_user_data = {
            "email": f"batch_expeditors_{int(time.time())}@expeditors.com",
            "username": f"expeditors_batch_{int(time.time())}",
            "password": "ExpeditorsTest123!",
            "first_name": "Expeditors",
            "last_name": "BatchTester"
        }

    def log(self, message):
        """Log test messages with timestamp"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")

    def run_test(self, name, method, endpoint, expected_status, data=None, files=None, timeout=60, auth_required=False):
        """Run a single API test"""
        url = f"{self.api_url}/{endpoint}" if endpoint else f"{self.api_url}/"
        headers = {'Content-Type': 'application/json'} if not files else {}
        
        # Add authentication header if required and available
        if auth_required and self.auth_token:
            headers['Authorization'] = f'Bearer {self.auth_token}'

        self.tests_run += 1
        self.log(f"üîç Testing {name}...")
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=headers, timeout=timeout)
            elif method == 'POST':
                if files:
                    # Remove Content-Type for file uploads
                    if 'Content-Type' in headers:
                        del headers['Content-Type']
                    response = requests.post(url, data=data, files=files, headers=headers, timeout=timeout)
                else:
                    response = requests.post(url, json=data, headers=headers, timeout=timeout)

            success = response.status_code == expected_status
            if success:
                self.tests_passed += 1
                self.log(f"‚úÖ {name} - Status: {response.status_code}")
                try:
                    return True, response.json()
                except:
                    return True, {"message": "Success but no JSON response"}
            else:
                self.log(f"‚ùå {name} - Expected {expected_status}, got {response.status_code}")
                try:
                    error_data = response.json()
                    self.log(f"   Error details: {error_data}")
                except:
                    self.log(f"   Response text: {response.text[:500]}")
                return False, {}

        except Exception as e:
            self.log(f"‚ùå {name} - Error: {str(e)}")
            return False, {}

    def setup_authentication(self):
        """Setup authentication for both regular and Expeditors users"""
        self.log("üîê Setting up authentication...")
        
        # Register regular user
        success, response = self.run_test(
            "Register Regular User",
            "POST",
            "auth/register",
            200,
            data=self.test_user_data
        )
        if success:
            self.auth_token = response.get('access_token')
            self.log(f"   Regular user token: {'‚úÖ' if self.auth_token else '‚ùå'}")
        
        # Register Expeditors user
        success, response = self.run_test(
            "Register Expeditors User",
            "POST",
            "auth/register",
            200,
            data=self.expeditors_user_data
        )
        if success:
            self.expeditors_token = response.get('access_token')
            self.log(f"   Expeditors user token: {'‚úÖ' if self.expeditors_token else '‚ùå'}")
        
        return self.auth_token is not None

    def create_note_with_content(self, title, content, kind="photo", use_expeditors=False):
        """Create a note and add realistic content to it"""
        # Switch to appropriate token
        original_token = self.auth_token
        if use_expeditors and self.expeditors_token:
            self.auth_token = self.expeditors_token
        
        # Create note
        success, response = self.run_test(
            f"Create Note: {title}",
            "POST",
            "notes",
            200,
            data={"title": title, "kind": kind},
            auth_required=True
        )
        
        if not success:
            self.auth_token = original_token
            return None
        
        note_id = response.get('id')
        if not note_id:
            self.auth_token = original_token
            return None
        
        self.created_notes.append(note_id)
        
        # Add content by directly updating the note's artifacts
        # We'll simulate this by creating a note and then manually adding content via the database
        # For testing purposes, we'll use the note creation and then simulate content
        
        # Wait a moment for note creation to complete
        time.sleep(1)
        
        # Restore original token
        self.auth_token = original_token
        return note_id

    def add_mock_content_to_note(self, note_id, content):
        """Add mock content to a note by simulating processed content"""
        # This is a helper method to simulate content being added to notes
        # In a real scenario, this would happen through file upload and processing
        # For testing, we'll create notes and assume they have content
        self.log(f"   üìù Mock content added to note {note_id[:8]}...")
        return True

    def create_test_notes_with_realistic_content(self, use_expeditors=False):
        """Create multiple test notes with realistic business content"""
        self.log("üìù Creating test notes with realistic content...")
        
        # Business meeting content
        meeting_note = self.create_note_with_content(
            "Q4 Strategy Planning Meeting",
            """
            Executive team discussed Q4 strategic initiatives including market expansion, 
            cost optimization, and technology investments. Key decisions made on budget 
            allocation and resource planning. Action items assigned for implementation 
            timeline and success metrics definition.
            """,
            kind="audio",
            use_expeditors=use_expeditors
        )
        
        # Project status content
        project_note = self.create_note_with_content(
            "Project Alpha Status Update",
            """
            Project Alpha is 75% complete with delivery scheduled for next month. 
            Technical challenges identified in integration phase. Risk mitigation 
            strategies discussed including additional testing and stakeholder communication. 
            Budget remains on track with minor adjustments needed.
            """,
            kind="photo",
            use_expeditors=use_expeditors
        )
        
        # Market analysis content
        market_note = self.create_note_with_content(
            "Market Analysis Report",
            """
            Market research indicates strong growth potential in emerging markets. 
            Competitive analysis shows opportunities for differentiation through 
            innovation and customer service excellence. Regulatory considerations 
            and compliance requirements identified for international expansion.
            """,
            kind="photo",
            use_expeditors=use_expeditors
        )
        
        # Customer feedback content
        customer_note = self.create_note_with_content(
            "Customer Feedback Session",
            """
            Customer satisfaction survey results show 85% positive feedback. 
            Key improvement areas identified in response time and product features. 
            Customer retention strategies discussed including loyalty programs 
            and enhanced support services. Implementation roadmap defined.
            """,
            kind="audio",
            use_expeditors=use_expeditors
        )
        
        notes = [meeting_note, project_note, market_note, customer_note]
        valid_notes = [note for note in notes if note is not None]
        
        self.log(f"   Created {len(valid_notes)} test notes successfully")
        return valid_notes

    def test_batch_report_basic_functionality(self, note_ids, use_expeditors=False):
        """Test basic batch report generation"""
        # Switch to appropriate token
        original_token = self.auth_token
        if use_expeditors and self.expeditors_token:
            self.auth_token = self.expeditors_token
        
        success, response = self.run_test(
            f"Batch Report Generation ({'Expeditors' if use_expeditors else 'Regular'})",
            "POST",
            "notes/batch-report",
            200,
            data=note_ids[:3],  # Use first 3 notes
            auth_required=True,
            timeout=90
        )
        
        if success:
            report = response.get('report', '')
            title = response.get('title', '')
            source_notes = response.get('source_notes', [])
            note_count = response.get('note_count', 0)
            is_expeditors = response.get('is_expeditors', False)
            
            self.log(f"   üìä Report generated successfully")
            self.log(f"   üìù Report length: {len(report)} characters")
            self.log(f"   üìã Title: {title}")
            self.log(f"   üìÑ Source notes: {note_count}")
            self.log(f"   üè¢ Expeditors user: {is_expeditors}")
            
            # Verify report content quality
            if len(report) > 3000:
                self.log(f"   ‚úÖ Report has substantial content ({len(report)} chars)")
            else:
                self.log(f"   ‚ö†Ô∏è  Report may be too short ({len(report)} chars)")
            
            # Check for required sections
            required_sections = [
                'EXECUTIVE SUMMARY',
                'COMPREHENSIVE ANALYSIS', 
                'STRATEGIC RECOMMENDATIONS',
                'IMPLEMENTATION ROADMAP'
            ]
            
            sections_found = 0
            for section in required_sections:
                if section in report:
                    sections_found += 1
                    self.log(f"   ‚úÖ Found section: {section}")
                else:
                    self.log(f"   ‚ùå Missing section: {section}")
            
            self.log(f"   üìä Sections found: {sections_found}/{len(required_sections)}")
            
            # Check for clean formatting (no markdown symbols)
            has_markdown = '###' in report or '**' in report or '```' in report
            if not has_markdown:
                self.log(f"   ‚úÖ Clean formatting (no markdown symbols)")
            else:
                self.log(f"   ‚ùå Contains markdown symbols - formatting not clean")
            
            # Check for professional bullet points
            if '‚Ä¢ ' in report:
                bullet_count = report.count('‚Ä¢ ')
                self.log(f"   ‚úÖ Professional bullet points found: {bullet_count}")
            else:
                self.log(f"   ‚ùå No professional bullet points found")
            
            # Check Expeditors branding
            if use_expeditors:
                if 'EXPEDITORS INTERNATIONAL' in report:
                    self.log(f"   ‚úÖ Expeditors branding detected")
                else:
                    self.log(f"   ‚ùå Expeditors branding missing")
            
            # Verify source notes are included
            if note_count == len(note_ids[:3]):
                self.log(f"   ‚úÖ Correct number of source notes processed")
            else:
                self.log(f"   ‚ùå Note count mismatch: expected {len(note_ids[:3])}, got {note_count}")
        
        # Restore original token
        self.auth_token = original_token
        return success, response if success else {}

    def test_batch_report_error_handling(self):
        """Test batch report error handling scenarios"""
        self.log("üö® Testing batch report error handling...")
        
        # Test with empty note list
        success, response = self.run_test(
            "Batch Report - Empty List (Should Fail)",
            "POST",
            "notes/batch-report",
            400,
            data=[],
            auth_required=True
        )
        
        # Test with invalid note IDs
        success2, response2 = self.run_test(
            "Batch Report - Invalid Note IDs (Should Handle Gracefully)",
            "POST",
            "notes/batch-report",
            400,  # Should return 400 if no valid notes found
            data=["invalid-id-1", "invalid-id-2", "invalid-id-3"],
            auth_required=True
        )
        
        # Test with mixed valid/invalid note IDs
        if self.created_notes:
            mixed_ids = [self.created_notes[0], "invalid-id", self.created_notes[1] if len(self.created_notes) > 1 else "invalid-id-2"]
            success3, response3 = self.run_test(
                "Batch Report - Mixed Valid/Invalid IDs",
                "POST",
                "notes/batch-report",
                200,  # Should succeed with valid notes only
                data=mixed_ids,
                auth_required=True,
                timeout=90
            )
            
            if success3:
                note_count = response3.get('note_count', 0)
                self.log(f"   ‚úÖ Processed {note_count} valid notes from mixed list")
        
        return success and success2

    def test_batch_report_content_processing(self, note_ids):
        """Test that batch reports properly combine content from multiple notes"""
        self.log("üîÑ Testing batch report content processing...")
        
        # Generate batch report
        success, response = self.run_test(
            "Batch Report - Content Processing Test",
            "POST",
            "notes/batch-report",
            200,
            data=note_ids,
            auth_required=True,
            timeout=90
        )
        
        if success:
            report = response.get('report', '')
            source_notes = response.get('source_notes', [])
            note_count = response.get('note_count', 0)
            
            # Verify content synthesis
            self.log(f"   üìä Report synthesized from {note_count} notes")
            self.log(f"   üìù Combined report length: {len(report)} characters")
            
            # Check for synthesis indicators
            synthesis_indicators = [
                'across all content',
                'combined analysis',
                'multiple sources',
                'comprehensive review',
                'integrated approach'
            ]
            
            synthesis_found = 0
            for indicator in synthesis_indicators:
                if indicator.lower() in report.lower():
                    synthesis_found += 1
            
            if synthesis_found > 0:
                self.log(f"   ‚úÖ Content synthesis detected ({synthesis_found} indicators)")
            else:
                self.log(f"   ‚ö†Ô∏è  Limited synthesis indicators found")
            
            # Verify report is longer than individual reports would be
            expected_min_length = note_count * 800  # Rough estimate
            if len(report) >= expected_min_length:
                self.log(f"   ‚úÖ Report length appropriate for {note_count} notes")
            else:
                self.log(f"   ‚ö†Ô∏è  Report may be shorter than expected for {note_count} notes")
            
            # Check for strategic analysis sections
            strategic_sections = [
                'STRATEGIC RECOMMENDATIONS',
                'IMPLEMENTATION ROADMAP',
                'SUCCESS METRICS',
                'RISK ASSESSMENT'
            ]
            
            strategic_found = sum(1 for section in strategic_sections if section in report)
            self.log(f"   üìà Strategic sections found: {strategic_found}/{len(strategic_sections)}")
            
        return success

    def test_batch_report_openai_integration(self, note_ids):
        """Test OpenAI integration for batch processing"""
        self.log("ü§ñ Testing OpenAI integration for batch reports...")
        
        start_time = time.time()
        
        success, response = self.run_test(
            "Batch Report - OpenAI Integration Test",
            "POST",
            "notes/batch-report",
            200,
            data=note_ids[:2],  # Use 2 notes to test integration
            auth_required=True,
            timeout=120  # Longer timeout for AI processing
        )
        
        processing_time = time.time() - start_time
        
        if success:
            report = response.get('report', '')
            
            self.log(f"   ‚è±Ô∏è  Processing time: {processing_time:.1f} seconds")
            
            # Verify AI-generated content quality
            ai_quality_indicators = [
                'analysis',
                'insights',
                'recommendations',
                'strategic',
                'implementation',
                'assessment'
            ]
            
            quality_score = sum(1 for indicator in ai_quality_indicators if indicator.lower() in report.lower())
            self.log(f"   üéØ AI quality indicators: {quality_score}/{len(ai_quality_indicators)}")
            
            # Check processing time is reasonable
            if processing_time < 60:
                self.log(f"   ‚úÖ Processing completed within reasonable time")
            elif processing_time < 120:
                self.log(f"   ‚ö†Ô∏è  Processing took longer than expected but completed")
            else:
                self.log(f"   ‚ùå Processing took too long")
            
            # Verify content coherence
            if len(report) > 2000 and quality_score >= 4:
                self.log(f"   ‚úÖ AI-generated content appears coherent and comprehensive")
            else:
                self.log(f"   ‚ö†Ô∏è  AI-generated content may need review")
        
        return success

    def test_batch_report_different_content_types(self):
        """Test batch reports with different content types (audio, photo, mixed)"""
        self.log("üé≠ Testing batch reports with different content types...")
        
        if len(self.created_notes) < 3:
            self.log("   ‚ö†Ô∏è  Not enough notes for content type testing")
            return False
        
        # Test with mixed content types
        mixed_notes = self.created_notes[:3]
        success, response = self.run_test(
            "Batch Report - Mixed Content Types",
            "POST",
            "notes/batch-report",
            200,
            data=mixed_notes,
            auth_required=True,
            timeout=90
        )
        
        if success:
            report = response.get('report', '')
            note_count = response.get('note_count', 0)
            
            self.log(f"   üìä Mixed content report generated: {len(report)} characters")
            self.log(f"   üìÑ Notes processed: {note_count}")
            
            # Check for content type handling
            content_indicators = [
                'meeting',
                'project',
                'analysis',
                'feedback',
                'discussion',
                'review'
            ]
            
            indicators_found = sum(1 for indicator in content_indicators if indicator.lower() in report.lower())
            self.log(f"   üéØ Content type indicators: {indicators_found}/{len(content_indicators)}")
            
            if indicators_found >= 3:
                self.log(f"   ‚úÖ Successfully processed mixed content types")
            else:
                self.log(f"   ‚ö†Ô∏è  Limited content type diversity detected")
        
        return success

    def run_comprehensive_batch_report_test(self):
        """Run comprehensive batch report functionality tests"""
        self.log("üöÄ Starting Batch Report Functionality Tests")
        self.log(f"   Base URL: {self.base_url}")
        self.log(f"   API URL: {self.api_url}")
        
        # Setup authentication
        if not self.setup_authentication():
            self.log("‚ùå Authentication setup failed - stopping tests")
            return False
        
        # Create test notes with realistic content
        regular_notes = self.create_test_notes_with_realistic_content(use_expeditors=False)
        expeditors_notes = self.create_test_notes_with_realistic_content(use_expeditors=True)
        
        if len(regular_notes) < 2:
            self.log("‚ùå Failed to create sufficient test notes - stopping tests")
            return False
        
        # === PRIMARY BATCH REPORT TESTS ===
        self.log("\nüìä PRIMARY BATCH REPORT TESTS")
        
        # Test 1: Basic batch report functionality
        success1, report_data = self.test_batch_report_basic_functionality(regular_notes, use_expeditors=False)
        
        # Test 2: Expeditors batch report with branding
        if expeditors_notes and len(expeditors_notes) >= 2:
            success2, expeditors_report = self.test_batch_report_basic_functionality(expeditors_notes, use_expeditors=True)
        else:
            success2 = True  # Skip if no Expeditors notes
            self.log("   ‚ö†Ô∏è  Skipping Expeditors batch report test (no notes)")
        
        # Test 3: Error handling
        success3 = self.test_batch_report_error_handling()
        
        # Test 4: Content processing verification
        success4 = self.test_batch_report_content_processing(regular_notes)
        
        # Test 5: OpenAI integration testing
        success5 = self.test_batch_report_openai_integration(regular_notes)
        
        # Test 6: Different content types
        success6 = self.test_batch_report_different_content_types()
        
        # === SPECIFIC VERIFICATION TESTS ===
        self.log("\nüîç SPECIFIC VERIFICATION TESTS")
        
        # Test batch report with exactly 2-3 notes as requested
        if len(regular_notes) >= 3:
            success7, verification_report = self.test_batch_report_basic_functionality(regular_notes[:3], use_expeditors=False)
            if success7:
                report = verification_report.get('report', '')
                self.log(f"   ‚úÖ 3-note batch report: {len(report)} characters")
                
                # Verify all requested verification points
                verification_points = {
                    "Returns 200 OK": verification_report.get('note_count', 0) > 0,
                    "Contains content from all notes": verification_report.get('note_count', 0) == 3,
                    "Professional formatting": '‚Ä¢ ' in report and 'EXECUTIVE SUMMARY' in report,
                    "Substantial content": len(report) > 3000,
                    "Clean structure": '###' not in report and '**' not in report
                }
                
                for point, passed in verification_points.items():
                    status = "‚úÖ" if passed else "‚ùå"
                    self.log(f"   {status} {point}")
        else:
            success7 = True
            self.log("   ‚ö†Ô∏è  Skipping 3-note verification (insufficient notes)")
        
        all_tests_passed = all([success1, success2, success3, success4, success5, success6, success7])
        
        return all_tests_passed

    def print_summary(self):
        """Print test summary"""
        self.log("\n" + "="*60)
        self.log("üìä BATCH REPORT TEST SUMMARY")
        self.log("="*60)
        self.log(f"Tests run: {self.tests_run}")
        self.log(f"Tests passed: {self.tests_passed}")
        self.log(f"Tests failed: {self.tests_run - self.tests_passed}")
        self.log(f"Success rate: {(self.tests_passed/self.tests_run*100):.1f}%" if self.tests_run > 0 else "0%")
        
        if self.created_notes:
            self.log(f"\nCreated test notes: {len(self.created_notes)}")
            for i, note_id in enumerate(self.created_notes[:5]):  # Show first 5
                self.log(f"  {i+1}. {note_id}")
            if len(self.created_notes) > 5:
                self.log(f"  ... and {len(self.created_notes) - 5} more")
        
        self.log("="*60)
        
        return self.tests_passed == self.tests_run

def main():
    """Main test execution"""
    tester = BatchReportTester()
    
    try:
        success = tester.run_comprehensive_batch_report_test()
        all_passed = tester.print_summary()
        
        if success and all_passed:
            print("\nüéâ All batch report tests passed! Functionality is working correctly.")
            return 0
        else:
            print(f"\n‚ö†Ô∏è  Some batch report tests failed. Check the logs above for details.")
            return 1
            
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Tests interrupted by user")
        tester.print_summary()
        return 1
    except Exception as e:
        print(f"\nüí• Unexpected error: {str(e)}")
        tester.print_summary()
        return 1

if __name__ == "__main__":
    sys.exit(main())